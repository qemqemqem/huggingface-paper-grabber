Papersarxiv:2505.11594SageAttention3: Microscaling FP4 Attention for Inference and An
  Exploration of 8-Bit TrainingPublished on May 16·Submitted byjt-zhangon May 21#2 Paper of the dayUpvote48+40Authors:Jintao Zhang,Jia Wei,Pengle Zhang,Xiaoming Xu,Haofeng Huang,Haoxu Wang,Kai Jiang,Jun Zhu,Jianfei ChenThe efficiency ofattentionis important due to its quadratic time
complexity. We enhance the efficiency ofattentionthrough two key
contributions: First, we leverage the newFP4 Tensor CoresinBlackwell GPUsto
accelerateattentioncomputation. Our implementation achieves 1038TOPSonRTX5090, which is a 5x speedup over the fastestFlashAttentiononRTX5090.
Experiments show that our FP4attentioncan accelerate inference of various
models in a plug-and-play way. Second, we pioneerlow-bit attentionto training
tasks. Existinglow-bit attentionworks likeFlashAttention3 and SageAttentionfocus only on inference. However, the efficiency of training large models is
also important. To explore whetherlow-bit attentioncan be effectively applied
to training tasks, we design an accurate and efficient 8-bitattentionfor both
forward andbackward propagation. Experiments indicate that 8-bitattentionachieves lossless performance infine-tuningtasks but exhibits slower
convergence inpretrainingtasks. The code will be available at
https://github.com/thu-ml/SageAttention.View arXiv pageView PDFProject pageGitHub repositoryAdd to collectionCommunityjt-zhangPaper authorPaper submitterabout 22 hours agoSageAttention3: Microscaling FP4 Attention for inference with a 5x speedup and an 8-bit Attention for Training.The code will be available athttps://github.com/thu-ml/SageAttention.See translationReplyEditPreviewUpload images, audio, and videos by dragging in the text input, pasting, orclicking here.Tap or paste here to upload imagesComment·Sign uporlog into comment