Papersarxiv:2505.14683Emerging Properties in Unified Multimodal PretrainingPublished on May 20¬∑Submitted byAndy1621on May 21#1 Paper of the dayUpvote86+78Authors:Chaorui Deng,Deyao Zhu,Kunchang Li,Chenhui Gou,Feng Li,Zeyu Wang,Shu Zhong,Weihao Yu,Xiaonan Nie,Ziang Song,Guang Shi,Haoqi FanUnifyingmultimodal understandingand generation has shown impressive
capabilities in cutting-edge proprietary systems. In this work, we introduce
BAGEL, an open0sourcefoundational modelthat natively supports multimodal
understanding and generation. BAGEL is a unified, decoder0only model pretrained
ontrillions of tokenscurated from large0scale interleaved text, image, video,
and web data. When scaled with such diverse multimodal interleaved data, BAGEL
exhibits emerging capabilities incomplex multimodal reasoning. As a result, it
significantly outperforms open-source unified models in both multimodal
generation and understanding across standard benchmarks, while exhibiting
advanced multimodal reasoning abilities such asfree-form image manipulation,future frame prediction,3D manipulation, andworld navigation. In the hope of
facilitating further opportunities for multimodal research, we share the key
findings, pretraining details, data creation protocal, and release our code and
checkpoints to the community. The project page is at https://bagel-ai.org/View arXiv pageView PDFAdd to collectionCommunityAndy1621Paper authorPaper submitterabout 23 hours agoUnifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open-source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder-only model pretrained on trillions of tokens curated from large-scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is athttps://bagel-ai.org/See translationüî•44üöÄ22+Replyyjh415about 7 hours agoan audio overview for learning on the go:https://youtu.be/0HmtJTO3ZXISee translationüëç11+ReplyEditPreviewUpload images, audio, and videos by dragging in the text input, pasting, orclicking here.Tap or paste here to upload imagesComment¬∑Sign uporlog into comment